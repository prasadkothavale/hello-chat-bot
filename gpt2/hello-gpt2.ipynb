{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed48d27b-dcd0-4757-89c0-d82098bac11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasad/tf-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-06 11:44:39.717110: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 11:44:39.810852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 11:44:39.846541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 11:44:39.857556: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 11:44:39.914451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 11:44:40.557689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.optim import AdamW\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e4ea0-83e5-4bbb-b71f-0120f3a559a7",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4284883-6c70-4f7f-ac6c-e4ef33781502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content'],\n",
       "        num_rows: 6286\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files=os.path.join('../utils/investopedia-dictionary', \"*.txt\"))\n",
    "dataset = dataset.rename_column(\"text\", \"content\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aa612-c3cc-447c-9210-80295bcbf13a",
   "metadata": {},
   "source": [
    "## Initialize tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d089af-ba8d-49fc-90f6-6249d57a496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('./models/first_model')\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('./models/first_model')\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e891cd-0650-4ef0-b4a5-50212787f399",
   "metadata": {},
   "source": [
    "## Tokenize and encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0b771e-9122-47f6-9e10-fb13be214d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 6286/6286 [00:20<00:00, 299.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6286\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"content\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e3ca63-b925-4757-bf2b-9a7d09008ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"input_ids\"] for item in batch]\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df517af-fede-490e-9b66-d1cec29c96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9928ab3d-ba2a-4bb9-9a24-a9083cf4f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0894d7d-9f0b-4c32-b62b-a35e7dd16a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 3.1339428424835205\n",
      "Epoch: 1, Step: 0, Loss: 3.335298538208008\n",
      "Epoch: 2, Step: 0, Loss: 3.1583783626556396\n",
      "Epoch: 3, Step: 0, Loss: 2.740562677383423\n",
      "Epoch: 4, Step: 0, Loss: 3.0632965564727783\n",
      "Epoch: 5, Step: 0, Loss: 3.5411484241485596\n",
      "Epoch: 6, Step: 0, Loss: 3.6008458137512207\n",
      "Epoch: 7, Step: 0, Loss: 3.497521162033081\n",
      "Epoch: 8, Step: 0, Loss: 3.605992078781128\n",
      "Epoch: 9, Step: 0, Loss: 3.1440649032592773\n",
      "Epoch: 10, Step: 0, Loss: 3.4177563190460205\n",
      "Epoch: 11, Step: 0, Loss: 3.604689121246338\n",
      "Epoch: 12, Step: 0, Loss: 3.021451711654663\n",
      "Epoch: 13, Step: 0, Loss: 3.9013915061950684\n",
      "Epoch: 14, Step: 0, Loss: 3.0129714012145996\n",
      "Epoch: 15, Step: 0, Loss: 3.345538377761841\n",
      "Epoch: 16, Step: 0, Loss: 3.5090811252593994\n",
      "Epoch: 17, Step: 0, Loss: 3.54833984375\n",
      "Epoch: 18, Step: 0, Loss: 3.663609027862549\n",
      "Epoch: 19, Step: 0, Loss: 3.430499315261841\n",
      "Epoch: 20, Step: 0, Loss: 2.8504245281219482\n",
      "Epoch: 21, Step: 0, Loss: 3.4634034633636475\n",
      "Epoch: 22, Step: 0, Loss: 3.4714345932006836\n",
      "Epoch: 23, Step: 0, Loss: 3.2093558311462402\n",
      "Epoch: 24, Step: 0, Loss: 3.443333625793457\n",
      "Epoch: 25, Step: 0, Loss: 3.710749864578247\n",
      "Epoch: 26, Step: 0, Loss: 3.344015598297119\n",
      "Epoch: 27, Step: 0, Loss: 3.3777706623077393\n",
      "Epoch: 28, Step: 0, Loss: 3.104952096939087\n",
      "Epoch: 29, Step: 0, Loss: 3.385885715484619\n",
      "Epoch: 30, Step: 0, Loss: 3.7183992862701416\n",
      "Epoch: 31, Step: 0, Loss: 3.5829389095306396\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "num_epochs=32\n",
    "for epoch in range(num_epochs):\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        if step%400==0:\n",
    "            print(\"Epoch: {}, Step: {}, Loss: {}\".format(epoch, step, loss.item()))\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4934596-f1e9-45d6-953f-af80cdab406e",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "845fb7ea-0604-4fc4-8d49-f7a9badf863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is IPO?\n",
      "\n",
      "IPOs are a system of private equity offering to investors. A company, like a bank or a corporation, would own a stake in a new investment company, but would then buy up stock in that company. The public owns stakes in that company, and investors own shares in that company. The company's share price drops as its stock price goes down. Investors then sell those shares. This strategy is known as profit sharing. The IPO goes on.\n",
      "\n",
      "IPOs are different from money markets. We're not talking about just a particular type of money market, we're talking about a real market. Many\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is IPO?\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(device) # Move inputs to CPU / CUDA\n",
    "\n",
    "model = model.to(device)  # Move the model to CPU / CUDA\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=128,\n",
    ")\n",
    "print(tokenizer.batch_decode(gen_tokens)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d82f95-556c-4b35-877e-6a87945ffcab",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03ec8ab4-9bf2-4901-b027-167187ca9325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/first_model/tokenizer_config.json',\n",
       " './models/first_model/special_tokens_map.json',\n",
       " './models/first_model/vocab.json',\n",
       " './models/first_model/merges.txt',\n",
       " './models/first_model/added_tokens.json')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/first_model\")\n",
    "tokenizer.save_pretrained(\"./models/first_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461485b-3b05-4673-9728-627b22f29fbc",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf2e8587-5c84-4378-b588-5960ee06df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
