{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feca3d13-4beb-43ae-be4a-9b7294008247",
   "metadata": {},
   "source": [
    "# Investopedia Dictonary Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c33ccd2-a588-4e46-9d40-34fd165c05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3fe075-e227-43bc-86a3-65f16051b90a",
   "metadata": {},
   "source": [
    "## Define the URL and headers for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f799a559-8741-4827-bfd7-57b4fb02e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.investopedia.com/financial-term-dictionary-4769738\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad586322-d22d-487a-be9a-05934e59d86a",
   "metadata": {},
   "source": [
    "## Scrape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b33ade-66b2-4a16-891f-93ef54c37ce6",
   "metadata": {},
   "source": [
    "### 1. Scrape Dictionary Term Links \n",
    "We first scrape all `<a>` tags with class `dictionary-top24-list__complete-listing-link` in the `div` with ID `dictionary-top24-list_1-0` and save the `href` attributes in `main_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af71213-96f5-4e7d-8aff-c455a2837ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 main links\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "main_links = []\n",
    "for a_tag in soup.select(\"div#dictionary-top24-list_1-0 a.dictionary-top24-list__complete-listing-link\"):\n",
    "    main_links.append(a_tag['href'])\n",
    "print(f'Found {len(main_links)} main links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd2de7-9c87-402b-b870-f94e0fa33eee",
   "metadata": {},
   "source": [
    "### 2. Scrape Links for Words From Each Term Page\n",
    "Define `scrape_dictionary_links()` to handle the parsing of each secondary page, collecting all relevant links within `<div id=\"dictionary-top300-list_1-0\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d72d4c-9dc4-4e18-b08e-933d7b27c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping initial links: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 13.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# scrape links from each dictionary term page\n",
    "def scrape_dictionary_links(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract links from the specified div in each page\n",
    "        for link in soup.select('div#dictionary-top300-list_1-0 a'):\n",
    "            text = link.get_text(strip=True)\n",
    "            href = link['href']\n",
    "            dictionary_links[text] = href\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "dictionary_links = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = [executor.submit(scrape_dictionary_links, url) for url in main_links]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping initial links\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118665cf-fa2f-49e0-a551-428a487e4cd6",
   "metadata": {},
   "source": [
    "### 3. Scrape Article Content From Each Term's Page\n",
    "Define `scrape_article_content()` to fetch the actual article text in each term's page, saving the content to a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2fdc94-9885-4380-aabe-3b0121acb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping article content: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 6317/6317 [08:11<00:00, 12.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to clean text\n",
    "def clean_text(text):\n",
    "    # Normalize Unicode to ASCII\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Replace any kind of hyphen, dash, or special dash characters with a standard dash\n",
    "    text = re.sub(r'[\\u2010-\\u2015]', '-', text)\n",
    "    # Replace multiple spaces/newlines with single spaces/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Remove any non-printable characters and trim leading/trailing whitespace\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text).strip()\n",
    "    return text\n",
    "\n",
    "# Helper function to clean the filename by replacing non-alphanumeric characters with underscores\n",
    "def clean_filename(name):\n",
    "    return re.sub(r'[^\\w\\s-]', '_', name).strip()\n",
    "\n",
    "# Function to scrape article content from each term's page\n",
    "def scrape_article_content(name, url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the article content div\n",
    "        article_div = soup.find('div', class_='article-content')\n",
    "        if article_div:\n",
    "            # Extract and clean text content\n",
    "            article_text = article_div.get_text(strip=True)\n",
    "            cleaned_text = clean_text(article_text)\n",
    "            \n",
    "            # Save cleaned text to file\n",
    "            filename = f\"./investopedia-dictionary/{clean_filename(name)}.txt\"\n",
    "            with open(filename, 'w', encoding='ascii') as file:\n",
    "                file.write(cleaned_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape article {name}: {e}\")\n",
    "\n",
    "Path(\"./investopedia-dictionary\").mkdir(parents=True, exist_ok=True)\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = [executor.submit(scrape_article_content, name, url) for name, url in dictionary_links.items()]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping article content\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d6c76-fd08-4c47-86fe-f7fb39c55283",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Checks if the content starts with \"Close\" and, if so, removes it and any subsequent whitespace.\n",
    "2. Adds space after first `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a83400-0924-4ba9-bbca-ebc3dce210e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 6286/6286 [00:00<00:00, 16244.31file/s]\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"./investopedia-dictionary\"\n",
    "\n",
    "# Function to clean the content\n",
    "def clean_file_content(content):\n",
    "    # Remove any \"Close\" text at the beginning of the file\n",
    "    if content.startswith(\"Close\"):\n",
    "        content = content[5:].lstrip()  # Remove \"Close\" and any following whitespace\n",
    "    elif content.startswith(\"Definition\"):\n",
    "        content = content[10:].lstrip()\n",
    "    \n",
    "    # Ensure there's a space after the first question mark if missing\n",
    "    content = re.sub(r'\\?(\\S)', r'? \\1', content, count=1)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Get a list of all text files in the directory\n",
    "text_files = [f for f in os.listdir(DIRECTORY) if f.endswith(\".txt\")]\n",
    "\n",
    "# Iterate through all text files with progress tracking\n",
    "for filename in tqdm(text_files, desc=\"Cleaning files\", unit=\"file\"):\n",
    "    file_path = os.path.join(DIRECTORY, filename)\n",
    "    \n",
    "    # Read the original content\n",
    "    with open(file_path, 'r', encoding='ascii') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Clean the content\n",
    "    cleaned_content = clean_file_content(content)\n",
    "    \n",
    "    # Write the cleaned content back to the file\n",
    "    with open(file_path, 'w', encoding='ascii') as file:\n",
    "        file.write(cleaned_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
